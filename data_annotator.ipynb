{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "smm0fttwz8",
   "metadata": {},
   "source": [
    "# Clinical Trials LLM Annotation Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook is **Step 2** in our educational pipeline demonstrating how Large Language Models (LLMs) can enhance bioinformatics workflows. We'll use an LLM to intelligently annotate the clinical trials data we cleaned in `data_retriever.ipynb`.\n",
    "\n",
    "## What is an \"Agentic Workflow\"?\n",
    "\n",
    "You may have heard terms like \"AI agents\" or \"agentic systems\" - often associated with autonomous AI that can take actions independently. **This notebook demonstrates a different, more controlled approach**: a **circumscribed agentic workflow**.\n",
    "\n",
    "### Key Characteristics:\n",
    "\n",
    "**üß† Intelligent Decision-Making**\n",
    "- The LLM interprets medical terminology, handles synonyms, and extracts meaning from unstructured text\n",
    "- Goes beyond simple string matching to understand context\n",
    "\n",
    "**üõ°Ô∏è Strict Guardrails**\n",
    "- All LLM outputs are constrained to predefined formats (structured schemas)\n",
    "- Multiple validation layers prevent hallucinations\n",
    "- Original data is never modified\n",
    "\n",
    "**üîç Audit Trail**\n",
    "- Every LLM decision is logged\n",
    "- Results exported to CSV for human review\n",
    "- Fully transparent and reproducible\n",
    "\n",
    "**üë§ Human Oversight**\n",
    "- Optional human review and override capability\n",
    "- Red flags logged for manual inspection\n",
    "\n",
    "### What This Is NOT:\n",
    "\n",
    "‚ùå **Not** an autonomous agent that can take arbitrary actions  \n",
    "‚ùå **Not** a system that modifies or corrupts your raw data  \n",
    "‚ùå **Not** a black box - every decision is traceable  \n",
    "\n",
    "### What We'll Accomplish:\n",
    "\n",
    "This notebook enriches our clinical trials dataset by:\n",
    "\n",
    "1. **Mapping unmapped conditions** ‚Üí Finding MeSH terms for conditions that couldn't be automatically matched\n",
    "2. **Extracting primary conditions** ‚Üí Identifying the main disease studied in each trial from context\n",
    "3. **Categorizing therapeutically** ‚Üí Assigning trials to therapeutic areas (ONCOLOGY, CARDIOLOGY, etc.)\n",
    "\n",
    "By the end, each trial will have:\n",
    "- Standardized MeSH condition terms\n",
    "- Therapeutic category classification\n",
    "- Confidence scores for LLM decisions\n",
    "- Audit trail of all annotations\n",
    "\n",
    "Let's explore how to integrate LLM intelligence **purposefully and safely** into a bioinformatics data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594b9067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from langchain_openai import ChatOpenAI\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from services import logging_config, mesh_mapper, annotator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lduvyho8wq",
   "metadata": {},
   "source": [
    "## The Spectrum: Deterministic ‚Üí Circumscribed Agent ‚Üí Autonomous AI\n",
    "\n",
    "To understand what makes this workflow \"agentic but safe,\" let's compare three approaches to data processing:\n",
    "\n",
    "### Approach 1: Fully Deterministic Script\n",
    "\n",
    "**How it works:**\n",
    "```python\n",
    "if condition == \"T2DM\":\n",
    "    standardized = \"Type 2 Diabetes\"\n",
    "elif condition == \"Type II Diabetes\":\n",
    "    standardized = \"Type 2 Diabetes\"\n",
    "# ... must hardcode every possible variant\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Predictable and fast\n",
    "- ‚úÖ No risk of hallucination\n",
    "- ‚úÖ Easy to debug\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Brittle - fails on unexpected inputs\n",
    "- ‚ùå Can't handle abbreviations or synonyms it wasn't programmed for\n",
    "- ‚ùå Requires exhaustive enumeration of all possibilities\n",
    "- ‚ùå Can't understand context\n",
    "\n",
    "---\n",
    "\n",
    "### Approach 2: Circumscribed Agent (This Notebook)\n",
    "\n",
    "**How it works:**\n",
    "```python\n",
    "# LLM interprets context and maps to structured output\n",
    "result = llm.ask(\n",
    "    \"Map this condition to a MeSH term\",\n",
    "    structured_output=ConditionMapping  # Forces specific format\n",
    ")\n",
    "# Then validate against external database\n",
    "if validate_mesh_term(result.mesh_id):\n",
    "    accept(result)\n",
    "else:\n",
    "    reject_and_log(result)\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Handles synonyms, abbreviations, typos gracefully\n",
    "- ‚úÖ Understands context (e.g., \"diabetes in patients with...\" ‚Üí extracts \"diabetes\")\n",
    "- ‚úÖ Adapts to variations without reprogramming\n",
    "- ‚úÖ Structured outputs prevent free-form hallucination\n",
    "- ‚úÖ Validation layers catch errors\n",
    "- ‚úÖ Audit trail for every decision\n",
    "\n",
    "**Cons:**\n",
    "- ‚ö†Ô∏è Requires validation infrastructure\n",
    "- ‚ö†Ô∏è Slower than pure rule-based systems\n",
    "- ‚ö†Ô∏è Small risk of misinterpretation (mitigated by confidence scores)\n",
    "\n",
    "---\n",
    "\n",
    "### Approach 3: Fully Autonomous Agent\n",
    "\n",
    "**How it works:**\n",
    "```python\n",
    "# Agent has freedom to query databases, modify data, take actions\n",
    "agent.run(\"Improve the quality of this clinical trials dataset\")\n",
    "# Agent decides what to do with no constraints\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- ‚úÖ Maximum flexibility\n",
    "- ‚úÖ Can discover novel approaches\n",
    "\n",
    "**Cons:**\n",
    "- ‚ùå Unpredictable behavior\n",
    "- ‚ùå High risk of data corruption\n",
    "- ‚ùå Opaque decision-making\n",
    "- ‚ùå Difficult to reproduce results\n",
    "- ‚ùå May take unintended actions\n",
    "\n",
    "---\n",
    "\n",
    "## Why We Choose the Middle Ground\n",
    "\n",
    "For **bioinformatics workflows**, the circumscribed agent approach can offer the safest trade-off:\n",
    "\n",
    "| Requirement | Deterministic | Circumscribed Agent | Autonomous |\n",
    "|-------------|---------------|---------------------|------------|\n",
    "| Flexibility | ‚ùå | ‚úÖ | ‚úÖ |\n",
    "| Data Safety | ‚úÖ | ‚úÖ | ‚ùå |\n",
    "| Reproducibility | ‚úÖ | ‚úÖ | ‚ùå |\n",
    "| Context Understanding | ‚ùå | ‚úÖ | ‚úÖ |\n",
    "| Auditability | ‚úÖ | ‚úÖ | ‚ùå |\n",
    "| Validation | ‚úÖ | ‚úÖ | ‚ö†Ô∏è |\n",
    "\n",
    "**Result:** We get LLM intelligence (context understanding, synonym handling) **with** the safety and auditability of traditional scripts.\n",
    "\n",
    "---\n",
    "\n",
    "## Local LLM Configuration\n",
    "\n",
    "Notice in the code below that we're using a **local LLM** (configured via `LOCAL_LLM_URL` environment variable) rather than a cloud API like OpenAI or Anthropic.\n",
    "\n",
    "**Why local?**\n",
    "\n",
    "üîí **Data Privacy**\n",
    "- Clinical trial data may include proprietary information\n",
    "- Local inference means data never leaves your infrastructure\n",
    "- Critical for pharmaceutical/biotech companies with IP concerns\n",
    "\n",
    "üí∞ **Cost Control**\n",
    "- Annotating hundreds or thousands of trials requires many API calls\n",
    "- Local models (e.g., LLaMA, Mistral) run at zero marginal cost\n",
    "\n",
    "‚ö° **Speed**\n",
    "- No network latency\n",
    "\n",
    "**Trade-off:**\n",
    "- Local models may be slightly (or significantly) less capable than frontier models (GPT-5, Claude Sonnet)\n",
    "- But for structured tasks with validation, mid-tier models can perform well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61b95d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-13 08:30:03] INFO     - __main__ - Data will be saved at:/Users/joshuaziel/Documents/Coding/glp-1_landscape/data\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "\n",
    "logger = logging_config.get_logger(__name__)\n",
    "\n",
    "DATA_STORAGE = os.getenv(\"DATA_LOC\", None)\n",
    "\n",
    "if DATA_STORAGE and Path(DATA_STORAGE).exists():\n",
    "    logger.info(f\"Data will be saved at:{DATA_STORAGE}\")\n",
    "else:\n",
    "    DATA_STORAGE = Path(__file__).resolve()\n",
    "    logger.warning(f\"Warning: Data storage path in environment does not exist or was not set, saving data here: {DATA_STORAGE}\")\n",
    "\n",
    "cleaned_trials_loc = f\"{DATA_STORAGE}/cleaned_trials.pkl\"\n",
    "if Path(cleaned_trials_loc).exists():\n",
    "    with open(cleaned_trials_loc, \"rb\") as f:\n",
    "        cleaned_trials = pickle.load(f)\n",
    "else:\n",
    "    logger.warning(\"No pkl file found: You must run the data retriever workflow before executing this noteboook\")\n",
    "\n",
    "annotator_llm = ChatOpenAI(base_url=os.getenv(\"LOCAL_LLM_URL\"), model = os.getenv(\"LOCAL_LLM\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeep3u9w46c",
   "metadata": {},
   "source": [
    "## Data Safety Architecture\n",
    "\n",
    "Before we run the LLM annotation workflow, let's understand how this system **guarantees** your original data remains intact.\n",
    "\n",
    "### The \"Working Copy\" Pattern\n",
    "\n",
    "```\n",
    "Original Data (cleaned_trials.pkl)\n",
    "         ‚Üì\n",
    "    [Load into memory]\n",
    "         ‚Üì\n",
    "  Working Copy (in AnnotatorWorkflow)\n",
    "         ‚Üì\n",
    "   [LLM annotations applied]\n",
    "         ‚Üì\n",
    " Annotated Data (new output file)\n",
    "         ‚Üì\n",
    "Original file UNCHANGED ‚úÖ\n",
    "```\n",
    "\n",
    "### Protective Mechanisms\n",
    "\n",
    "**1. Immutable Original**\n",
    "- The `cleaned_trials.pkl` file from `data_retriever.ipynb` is **never modified**\n",
    "- It remains as a permanent checkpoint you can always return to\n",
    "\n",
    "**2. `original_data` Attribute**\n",
    "- When `AnnotatorWorkflow` loads your data, it stores a copy in `self.original_data`\n",
    "- All operations happen on `self.working_data` \n",
    "- You can always compare original vs annotated to see what changed\n",
    "\n",
    "**3. Non-Destructive Additions**\n",
    "- LLM doesn't overwrite existing columns\n",
    "- New information is **added** to:\n",
    "  - `matched_conditions` (appends MeSH terms)\n",
    "  - `tx_category` (new column)\n",
    "  - `tx_category_confidence` (new column)\n",
    "  - `llm_annotations` (tracks which fields were LLM-modified)\n",
    "\n",
    "**4. Timestamped Outputs**\n",
    "- All exports include timestamps in filenames (e.g., `annotated_trials_20231113_143022.pkl`)\n",
    "- Never overwrites previous runs\n",
    "- Full versioning history maintained\n",
    "\n",
    "**5. CSV Exports for Audit**\n",
    "- Every mapping decision exported to human-readable CSV:\n",
    "  - `mapped_to_existing_conditions.csv` - Synonym mappings\n",
    "  - `searched_conditions.csv` - New MeSH term searches\n",
    "  - `categorized_mesh_terms.csv` - Therapeutic classifications\n",
    "- These allow manual review of every LLM decision\n",
    "\n",
    "### Verification After Running\n",
    "\n",
    "After the workflow completes, you can verify data integrity:\n",
    "\n",
    "```python\n",
    "# Compare trial counts\n",
    "assert len(cleaned_trials) == len(annotated_trials)\n",
    "\n",
    "# Check original columns still exist\n",
    "assert all(col in annotated_trials.columns for col in cleaned_trials.columns)\n",
    "\n",
    "# Verify NCT IDs unchanged (primary key)\n",
    "assert (cleaned_trials['nct_id'] == annotated_trials['nct_id']).all()\n",
    "```\n",
    "\n",
    "### What If Something Goes Wrong?\n",
    "\n",
    "If LLM annotations are unsatisfactory:\n",
    "1. **Revert:** Delete the output pickle and re-run from `cleaned_trials.pkl`\n",
    "2. **Adjust:** Modify prompts or validation rules in `annotator.py`\n",
    "3. **Override:** Use the human review CSV (explained later)\n",
    "\n",
    "**Your original data is always safe.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o69hpaato4",
   "metadata": {},
   "source": [
    "## Three-Stage Annotation Workflow\n",
    "\n",
    "The `AnnotatorWorkflow.run_annotation_workflow()` method below executes three sequential stages. Each builds on the previous stage's results:\n",
    "\n",
    "---\n",
    "\n",
    "### Stage 1: Map Unmapped Conditions to Existing MeSH Terms\n",
    "\n",
    "**Problem:**  \n",
    "In `data_retriever.ipynb`, we used the `mesh_mapper` service to map conditions to MeSH terms via API lookup. However, some conditions failed to map because:\n",
    "- They use non-standard abbreviations (e.g., \"T2DM\", \"CAD\")\n",
    "- They're informal terms (e.g., \"high blood sugar\" instead of \"hyperglycemia\")\n",
    "- They include study parameters mixed with conditions (e.g., \"diabetes with A1C > 7%\")\n",
    "\n",
    "**LLM Solution:**  \n",
    "The LLM reviews unmapped conditions and attempts to match them to **existing MeSH terms already in the dataset**.\n",
    "\n",
    "**How it works:**\n",
    "1. Extract all conditions that lack MeSH mappings\n",
    "2. Get list of all successfully mapped MeSH terms from previous step\n",
    "3. Ask LLM: \"Which existing MeSH term best matches this unmapped condition?\"\n",
    "4. LLM returns: Matched term + confidence level (HIGH/MEDIUM/LOW)\n",
    "5. **Validation:** Check that LLM's suggested term actually exists in our MeSH list\n",
    "6. Accept valid matches; reject and log invalid ones\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Unmapped: \"T2DM\"\n",
    "LLM matches to existing: \"Diabetes Mellitus, Type 2 (MeSH ID:D003924)\"\n",
    "Confidence: HIGH\n",
    "Status: ‚úÖ Accepted (term exists in our dataset)\n",
    "```\n",
    "\n",
    "**Output:** `mapped_to_existing_conditions.csv` for review\n",
    "\n",
    "---\n",
    "\n",
    "### Stage 2: Search for New MeSH Terms\n",
    "\n",
    "**Problem:**  \n",
    "Some unmapped conditions don't match any existing MeSH term in our dataset (e.g., trial studying a rare disease). These need fresh MeSH lookups.\n",
    "\n",
    "**LLM + API Solution:**  \n",
    "For remaining unmapped conditions, we use a **two-step process**:\n",
    "\n",
    "**Step 2a: Extract Primary Condition from Context**\n",
    "- Many trial descriptions are complex: \"Type 2 Diabetes in patients with chronic kidney disease and obesity\"\n",
    "- LLM analyzes the full trial record (title, summary, outcomes, conditions list)\n",
    "- Extracts the **primary medical condition** being studied\n",
    "- Returns: Clean condition string suitable for MeSH database search\n",
    "\n",
    "**Step 2b: Verify via NCBI API**\n",
    "- Take LLM-extracted condition and query NCBI's MeSH database\n",
    "- Use the same `mesh_mapper` service from the retriever notebook\n",
    "- Filter to disease/disorder categories only (tree codes C or F)\n",
    "- If found: Accept; If not found: Mark as \"NOT DETERMINED\"\n",
    "\n",
    "**Why two-step?**\n",
    "- LLM is great at understanding context and extracting meaning\n",
    "- But only the authoritative NCBI database can confirm valid MeSH terms\n",
    "- Combining both gives flexibility + accuracy\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Original condition: \"Diabetes in patients with renal impairment\"\n",
    "LLM extracts: \"Diabetes Mellitus\"\n",
    "NCBI API returns: \"Diabetes Mellitus (MeSH ID:D003920)\"\n",
    "Status: ‚úÖ Accepted (verified by NCBI)\n",
    "```\n",
    "\n",
    "**Output:** `searched_conditions.csv` for review\n",
    "\n",
    "---\n",
    "\n",
    "### Stage 3: Classify Trials into Therapeutic Categories\n",
    "\n",
    "**Problem:**  \n",
    "For analysis and visualization, it's useful to group trials by therapeutic area:\n",
    "- ONCOLOGY (cancer)\n",
    "- ENDOCRINOLOGY (diabetes, thyroid)\n",
    "- CARDIOVASCULAR (heart disease, hypertension)\n",
    "- NEUROLOGY (Alzheimer's, Parkinson's)\n",
    "- etc.\n",
    "\n",
    "MeSH terms are very specific (e.g., \"Diabetes Mellitus, Type 2\"), but we want broad categories.\n",
    "\n",
    "**LLM Solution:**  \n",
    "For each trial's MeSH condition term(s), the LLM assigns a therapeutic category from a predefined list of 22 categories.\n",
    "\n",
    "**How it works:**\n",
    "1. Provide LLM with the trial's MeSH-standardized condition\n",
    "2. Provide definitions of all 22 therapeutic categories\n",
    "3. LLM selects the **single best-fit category**\n",
    "4. Returns: Category + confidence + reasoning\n",
    "5. **Constraint:** Must choose from enum (no free-form categories)\n",
    "\n",
    "**Categories available:**\n",
    "- ONCOLOGY\n",
    "- ENDOCRINOLOGY  \n",
    "- CARDIOVASCULAR\n",
    "- NEUROLOGY\n",
    "- IMMUNOLOGY\n",
    "- INFECTIOUS_DISEASE\n",
    "- RESPIRATORY\n",
    "- GASTROENTEROLOGY\n",
    "- NEPHROLOGY\n",
    "- RHEUMATOLOGY\n",
    "- DERMATOLOGY\n",
    "- OPHTHALMOLOGY\n",
    "- OTOLARYNGOLOGY\n",
    "- PSYCHIATRY\n",
    "- HEMATOLOGY\n",
    "- SURGERY\n",
    "- OBSTETRICS_GYNECOLOGY\n",
    "- PEDIATRICS\n",
    "- GERIATRICS\n",
    "- PAIN_MANAGEMENT\n",
    "- CRITICAL_CARE\n",
    "- OTHER\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "MeSH term: \"Diabetes Mellitus, Type 2 (MeSH ID:D003924)\"\n",
    "LLM assigns: ENDOCRINOLOGY\n",
    "Confidence: HIGH\n",
    "Reasoning: \"Diabetes is a metabolic/endocrine disorder\"\n",
    "```\n",
    "\n",
    "**Output:** `categorized_mesh_terms.csv` for review\n",
    "\n",
    "---\n",
    "\n",
    "### Progressive Refinement\n",
    "\n",
    "Notice the workflow is **progressive**:\n",
    "1. Try to match existing terms (fastest, most reliable)\n",
    "2. If that fails, extract and search for new terms (slower, but comprehensive)\n",
    "3. Finally, categorize everything for high-level analysis\n",
    "\n",
    "This minimizes API calls and ensures maximum data coverage.\n",
    "\n",
    "---\n",
    "\n",
    "### Execution Time\n",
    "\n",
    "Expect the workflow below to take **several minutes** depending on:\n",
    "- Number of trials\n",
    "- Number of unmapped conditions  \n",
    "- LLM inference speed\n",
    "- API rate limits\n",
    "\n",
    "Progress will be logged in real-time. Be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fa6cc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-13 08:30:06] INFO     - services.annotator - Loaded existing MeSH map containing 298 mappings as a <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "session = annotator.AnnotatorWorkflow(df = cleaned_trials, llm = annotator_llm, data_loc = DATA_STORAGE )\n",
    "session.run_annotation_workflow()\n",
    "annotated_trials = session.annotated_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbvmv1skyag",
   "metadata": {},
   "source": [
    "## How LLM Decisions Are Constrained: Structured Outputs\n",
    "\n",
    "The workflow above has completed (or is running). Let's examine **how** the LLM is prevented from \"hallucinating\" or producing invalid outputs.\n",
    "\n",
    "### The Problem with Free-Form LLM Responses\n",
    "\n",
    "If we asked an LLM a simple question without constraints:\n",
    "\n",
    "```python\n",
    "# ‚ùå DANGEROUS - No constraints\n",
    "response = llm.ask(\"What MeSH term matches 'T2DM'?\")\n",
    "# LLM might return:\n",
    "# \"The term T2DM refers to Type 2 Diabetes Mellitus, which is...\"\n",
    "# (free-form text, hard to parse, may include errors)\n",
    "```\n",
    "\n",
    "**Issues:**\n",
    "- Response format is unpredictable\n",
    "- No way to programmatically extract the MeSH ID\n",
    "- No confidence level\n",
    "- May include explanations mixed with data\n",
    "- Difficult to validate\n",
    "\n",
    "---\n",
    "\n",
    "### The Solution: Pydantic Schemas (Structured Outputs)\n",
    "\n",
    "Instead, we define **strict data schemas** using Pydantic that the LLM **must** conform to:\n",
    "\n",
    "```python\n",
    "# ‚úÖ SAFE - Structured output\n",
    "class ConditionMapping(BaseModel):\n",
    "    \"\"\"Schema that LLM must follow\"\"\"\n",
    "    original_condition: str\n",
    "    matched_mesh_term: str\n",
    "    confidence: Literal[\"HIGH\", \"MEDIUM\", \"LOW\"]\n",
    "    reasoning: str\n",
    "\n",
    "response = llm.ask(\n",
    "    \"What MeSH term matches 'T2DM'?\",\n",
    "    structured_output=ConditionMapping  # Forces this format\n",
    ")\n",
    "\n",
    "# Response is guaranteed to have these fields:\n",
    "print(response.original_condition)  # \"T2DM\"\n",
    "print(response.matched_mesh_term)   # \"Diabetes Mellitus, Type 2 (MeSH ID:D003924)\"\n",
    "print(response.confidence)          # \"HIGH\"\n",
    "print(response.reasoning)           # \"T2DM is standard abbreviation for...\"\n",
    "```\n",
    "\n",
    "### Schemas Used in This Workflow\n",
    "\n",
    "**1. `MedicalConditionFilter`** (Stage 1 filtering)\n",
    "```python\n",
    "class MedicalConditionFilter(BaseModel):\n",
    "    medical_conditions: list[str]  # Only actual diseases\n",
    "    non_medical: list[str]         # Study parameters to exclude\n",
    "```\n",
    "\n",
    "**2. `ConditionMapping`** (Stage 1 synonym matching)\n",
    "```python\n",
    "class ConditionMapping(BaseModel):\n",
    "    original_condition: str\n",
    "    matched_mesh_term: str\n",
    "    confidence: ConfidenceLevel  # Enum: HIGH/MEDIUM/LOW\n",
    "```\n",
    "\n",
    "**3. `ConditionExtraction`** (Stage 2 context extraction)\n",
    "```python\n",
    "class ConditionExtraction(BaseModel):\n",
    "    primary_condition: str\n",
    "    confidence: ConfidenceLevel\n",
    "    reasoning: str\n",
    "```\n",
    "\n",
    "**4. `TxCategoryAnnotation`** (Stage 3 categorization)\n",
    "```python\n",
    "class TxCategoryAnnotation(BaseModel):\n",
    "    therapeutic_category: Optional[TherapeuticCategory]  # Enum of 22 categories\n",
    "    confidence: Optional[ConfidenceLevel]\n",
    "    explanation: Optional[str]\n",
    "```\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "**Benefits of Structured Outputs:**\n",
    "\n",
    "‚úÖ **Predictable Format**\n",
    "- Every response has the same structure\n",
    "- Easy to parse programmatically\n",
    "- No need for regex or string manipulation\n",
    "\n",
    "‚úÖ **Type Safety**\n",
    "- `confidence` must match exactly the requirements of the schema, for example \"VERY HIGH\", \"HIGH\", \"MEDIUM\",\"LOW\" or \"VERY_LOW\" (not \"pretty sure\" or \"maybe\")\n",
    "- `therapeutic_category` must be from predefined enum (not any random category)\n",
    "\n",
    "‚úÖ **Validation Built-In**\n",
    "- Pydantic automatically validates types\n",
    "- Missing required fields cause errors (caught immediately)\n",
    "- Invalid enum values rejected\n",
    "\n",
    "‚úÖ **Self-Documentation**\n",
    "- Schema serves as clear specification for LLM\n",
    "- Reduces ambiguity in prompts\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Enum Constraints\n",
    "\n",
    "The therapeutic category must be one of exactly 22 options:\n",
    "\n",
    "```python\n",
    "class TherapeuticCategory(str, Enum):\n",
    "    ONCOLOGY = \"ONCOLOGY\"\n",
    "    ENDOCRINOLOGY = \"ENDOCRINOLOGY\"\n",
    "    CARDIOVASCULAR = \"CARDIOVASCULAR\"\n",
    "    # ... 19 more categories\n",
    "```\n",
    "\n",
    "If LLM tries to return `\"METABOLISM\"` (not in enum) ‚Üí **Validation error**, not accepted.\n",
    "\n",
    "This prevents the LLM from inventing categories.\n",
    "\n",
    "---\n",
    "\n",
    "### Confidence Scoring\n",
    "\n",
    "Notice every schema includes a `confidence` field. This is crucial because:\n",
    "\n",
    "**LLMs can self-assess uncertainty:**\n",
    "For Example:\n",
    "- `HIGH`: Strong evidence, clear match\n",
    "- `MEDIUM`: Reasonable match but some ambiguity  \n",
    "- `LOW`: Weak match, manual review recommended\n",
    "\n",
    "While we haven't fully implemented this here, in could allow for more nuanced downstream filtering:\n",
    "```python\n",
    "# Only use high-confidence annotations\n",
    "high_conf = annotated_trials[annotated_trials['tx_category_confidence'] == 'HIGH']\n",
    "```\n",
    "\n",
    "**Manual review prioritization:**\n",
    "- Focus human review on LOW confidence items\n",
    "- Trust HIGH confidence items (but still audit-able)\n",
    "\n",
    "---\n",
    "\n",
    "### What This Means for Data Quality\n",
    "\n",
    "Structured outputs + validation layers = **controlled intelligence**\n",
    "\n",
    "The LLM brings:\n",
    "- Context understanding\n",
    "- Synonym recognition  \n",
    "- Reasoning capability\n",
    "\n",
    "But is constrained by:\n",
    "- Predefined schemas\n",
    "- Type validation\n",
    "- External API verification\n",
    "- Confidence scoring (if implemented)\n",
    "\n",
    "**Result:** Flexible enough to handle edge cases, safe enough for production pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0qv5xmom47xp",
   "metadata": {},
   "source": [
    "## Multiple Validation Layers: Defense Against Hallucination\n",
    "\n",
    "Structured outputs are the first line of defense, but we employ **multiple validation layers** to catch errors:\n",
    "\n",
    "### Layer 1: Schema Validation (Pydantic)\n",
    "\n",
    "**What it catches:**\n",
    "- Wrong data types (e.g., string instead of list)\n",
    "- Missing required fields\n",
    "- Invalid enum values\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# LLM tries to return invalid confidence level\n",
    "response.confidence = \"very confident\"  # ‚ùå Not in enum\n",
    "# Pydantic raises ValidationError ‚Üí Request rejected\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Layer 2: Cross-Validation Against Existing Data\n",
    "\n",
    "**Stage 1 (Synonym Matching):** When LLM suggests a MeSH term match, we verify it exists in our dataset.\n",
    "\n",
    "```python\n",
    "if mapping.mesh_term in valid_mesh_terms:\n",
    "    validated_mappings[mapping.original_condition] = mapping.mesh_term\n",
    "else:\n",
    "    rejected_count += 1\n",
    "    logger.info(f\"Rejected invalid mapping: '{mapping.original_condition}' -> '{mapping.mesh_term}' (not in existing MeSH terms)\")\n",
    "\n",
    "    if rejected_count > 0:\n",
    "        logger.info(f\"Total rejected mappings: {rejected_count}\")\n",
    "```\n",
    "\n",
    "**Result:** LLM cannot invent MeSH terms that don't exist in our validated dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Layer 3: External API Verification (NCBI)\n",
    "\n",
    "**Stage 2 (New MeSH Searches):** For conditions requiring fresh lookups, we don't trust LLM alone.\n",
    "\n",
    "**Two-step verification:**\n",
    "\n",
    "1. **LLM extracts** the primary condition from context\n",
    "2. **NCBI API verifies** with a real MeSH term before anything is added.\n",
    "\n",
    "```python\n",
    "try:\n",
    "    result = mesh_mapper.search_mesh_term(condition, filter_diseases_only=True)\n",
    "\n",
    "    if result:\n",
    "        mesh_term = f\"{result['mesh_term']} (MeSH ID:{result['mesh_id']})\"\n",
    "        logger.info(f\"Found MeSH term for '{condition}': {mesh_term}\")\n",
    "        return mesh_term\n",
    "    else:\n",
    "        logger.warning(f\"No MeSH term found for '{condition}'\")\n",
    "        return \"NOT DETERMINED\"\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error searching for MeSH term for '{condition}': {e}\")\n",
    "        return \"NOT DETERMINED\"\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- LLM might extract a reasonable-sounding but non-existent term\n",
    "- NCBI is the **authoritative source** for MeSH terms\n",
    "- Only conditions that can be clearly matched to a MeSH term in NCBI's official database result in mapping\n",
    "\n",
    "---\n",
    "\n",
    "### Layer 4: Retry Logic with Error Handling\n",
    "\n",
    "**Stage 3 (Categorization):** If LLM fails to return valid structured output, we retry up to 4 times.\n",
    "\n",
    "**Why retry?**\n",
    "- Occasionally LLMs have transient failures (parsing errors, malformed JSON)\n",
    "- Retry gives LLM a second chance before marking as failed\n",
    "- After 4 attempts, we fail gracefully (mark as None) rather than crash\n",
    "\n",
    "---\n",
    "\n",
    "### Layer 5: Confidence-Based Filtering\n",
    "\n",
    "Every LLM decision includes a confidence score. To take the notebook further, you could capture these in the dataframe and filter post-hoc:\n",
    "\n",
    "```python\n",
    "# Only accept high-confidence therapeutic categories\n",
    "reliable = annotated_trials[\n",
    "    annotated_trials['tx_category_confidence'] == 'HIGH'\n",
    "]\n",
    "\n",
    "# Flag low-confidence items for human review\n",
    "needs_review = annotated_trials[\n",
    "    annotated_trials['tx_category_confidence'] == 'LOW'\n",
    "]\n",
    "```\n",
    "\n",
    "This allows graduated trust:\n",
    "For example:\n",
    "- HIGH or better confidence: Use automatically\n",
    "- MEDIUM confidence: Spot-check a sample\n",
    "- LOW or worse confidence: Manual review required\n",
    "\n",
    "---\n",
    "\n",
    "### Layer 6: Comprehensive Logging\n",
    "\n",
    "Every validation failure is logged:\n",
    "\n",
    "```python\n",
    "logger.warning(f\"Mapping could not be validated for {condition}\")\n",
    "logger.info(f\"Successfully matched {count} conditions\")\n",
    "logger.error(f\"Failed to extract condition from trial {nct_id}\")\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Audit trail of all decisions and failures\n",
    "- Debugging: Identify systematic issues\n",
    "- Quality metrics: Track error rates over time\n",
    "\n",
    "Check the logs after running to see:\n",
    "- How many conditions were successfully mapped\n",
    "- Which mappings were rejected\n",
    "- Why certain trials couldn't be annotated\n",
    "\n",
    "---\n",
    "\n",
    "### Layer 7: CSV Exports for Human Review\n",
    "\n",
    "**How to use:**\n",
    "1. Open CSV in Excel/spreadsheet software\n",
    "2. Review as the process goes along to identify any systematic errors\n",
    "3. Override via human review CSV (see next section)\n",
    "\n",
    "---\n",
    "\n",
    "## Summary: Defense in Depth\n",
    "\n",
    "| Layer | What It Does | What It Catches |\n",
    "|-------|--------------|-----------------|\n",
    "| 1. Schema validation | Enforces structure | Type errors, missing fields, invalid enums |\n",
    "| 2. Cross-validation | Checks against existing data | Hallucinated MeSH terms not in dataset |\n",
    "| 3. API verification | Checks against NCBI | Non-existent MeSH terms |\n",
    "| 4. Retry logic | Multiple attempts | Transient LLM failures |\n",
    "| 5. Confidence scoring (if implemented) | Self-assessment | Uncertain mappings |\n",
    "| 6. Logging | Records all events | Systematic errors, trends |\n",
    "| 7. CSV exports | Human review | Edge cases, quality assurance |\n",
    "\n",
    "**No single layer is perfect, but together they create a robust system.**\n",
    "\n",
    "Even if an error slips through one layer, subsequent layers catch it. This \"defense in depth\" approach ensures:\n",
    "- High accuracy\n",
    "- Transparent failures\n",
    "- Auditable decisions\n",
    "- Graceful degradation (never crashes, logs failures instead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nfq2tyvpymm",
   "metadata": {},
   "source": [
    "## Inspecting Results: Quality Assurance\n",
    "\n",
    "The workflow has completed and created `annotated_trials` DataFrame. Now let's verify the quality of LLM annotations.\n",
    "\n",
    "### Quick Quality Checks\n",
    "\n",
    "**1. Verify Data Integrity**\n",
    "```python\n",
    "# Check no trials were lost\n",
    "print(f\"Original trials: {len(cleaned_trials)}\")\n",
    "print(f\"Annotated trials: {len(annotated_trials)}\")\n",
    "assert len(cleaned_trials) == len(annotated_trials)\n",
    "\n",
    "# Check NCT IDs match (primary key unchanged)\n",
    "assert (cleaned_trials['nct_id'] == annotated_trials['nct_id']).all()\n",
    "```\n",
    "\n",
    "**2. Check Annotation Coverage**\n",
    "```python\n",
    "# How many trials have MeSH-mapped conditions?\n",
    "trials_with_mesh = annotated_trials['matched_conditions'].apply(\n",
    "    lambda x: len(x) > 0 if isinstance(x, list) else False\n",
    ")\n",
    "coverage = trials_with_mesh.sum() / len(annotated_trials) * 100\n",
    "print(f\"Trials with MeSH conditions: {coverage:.1f}%\")\n",
    "\n",
    "# How many have therapeutic categories?\n",
    "trials_with_category = annotated_trials[len(annotated_trials['tx_category'])>0]\n",
    "print(f\"Trials with therapeutic category: {trials_with_category.sum()} \n",
    "      ({trials_with_category.mean()*100:.1f}%)\"\n",
    "\n",
    "print(f\"\\nTrials needing manual review: {len(needs_review)}\")\n",
    "print(\"\\nReasons:\")\n",
    "print(f\"  - No category assigned: {annotated_trials['tx_category'].isna().sum()}\")\n",
    "print(f\"  - No MeSH conditions: {annotated_trials['matched_conditions'].apply(lambda x: len(x) == 0).sum()}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Review CSV Exports\n",
    "**These files are ready for Human review and verification.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6908bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_trials_loc = f\"{DATA_STORAGE}/<fill in pkl filename to load>\"\n",
    "if Path(cleaned_trials_loc).exists():\n",
    "    with open(annotated_trials_loc, \"rb\") as f:\n",
    "        cleaned_trials = pickle.load(f)\n",
    "else:\n",
    "    logger.error(\"Check the path to the pickle!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d97c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_review_path = f\"{DATA_STORAGE}/human_review.csv\"\n",
    "if Path(human_review_path).exists():\n",
    "    changes_df = pd.read_csv(human_review_path, header=0)\n",
    "    logger.info(\"Loaded changes file for human review\")\n",
    "else: \n",
    "    changes_df = None\n",
    "    logger.info(\"Store your changes file for human review at the appropriate location\")\n",
    "\n",
    "if changes_df is not None:\n",
    "    try:\n",
    "        for change in changes_df.itertuples():    \n",
    "            row = change.row\n",
    "            column = change.column\n",
    "            new_assignment = change.new_assignment\n",
    "            annotated_trials.loc[row, column] = new_assignment\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Warning - your changes could not be applied.  Perhaps the file was not appropriately formatted: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8t0avlvtced",
   "metadata": {},
   "source": [
    "## Human-in-the-Loop: Override LLM Decisions\n",
    "\n",
    "Even with multiple validation layers, you may identify incorrect LLM annotations during manual review. The human-in-the-loop override system allows you to correct these without re-running the entire workflow.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Human Override Is Important\n",
    "\n",
    "**LLMs are probabilistic, not perfect:**\n",
    "- May misinterpret context (e.g., trial about diabetes complications ‚Üí wrongly categorizes as NEPHROLOGY instead of ENDOCRINOLOGY)\n",
    "- May struggle with edge cases (e.g., trials involving multiple conditions)\n",
    "- May lack domain expertise (e.g., specific knowledge about rare diseases)\n",
    "\n",
    "**Human experts bring:**\n",
    "- Domain knowledge\n",
    "- Understanding of research context\n",
    "- Ability to resolve ambiguity based on trial purpose\n",
    "\n",
    "**The goal:** Combine LLM efficiency (handles 95% automatically) with human expertise (corrects the remaining 5%)\n",
    "\n",
    "---\n",
    "\n",
    "### How to Override LLM Decisions\n",
    "\n",
    "**Step 1: Identify Issues During Review**\n",
    "\n",
    "While reviewing the annotated trials or CSV exports, you may find errors:\n",
    "\n",
    "```python\n",
    "# Example: Review trials categorized as NEPHROLOGY\n",
    "nephrology_trials = annotated_trials[annotated_trials['tx_category'] == 'NEPHROLOGY']\n",
    "\n",
    "for idx, trial in nephrology_trials.iterrows():\n",
    "    print(f\"Row {idx}: {trial['nct_id']} - {trial['brief_title']}\")\n",
    "    print(f\"  Category: {trial['tx_category']} (confidence: {trial['tx_category_confidence']})\")\n",
    "    print(f\"  Conditions: {trial['matched_conditions']}\")\n",
    "    \n",
    "# You notice: Row 245 is a diabetes trial, should be ENDOCRINOLOGY, not NEPHROLOGY\n",
    "```\n",
    "\n",
    "**Step 2: Create a Human Review CSV**\n",
    "\n",
    "Create a file called `human_review.csv` in your `DATA_STORAGE` directory with this format:\n",
    "\n",
    "| row | column | new_assignment |\n",
    "|-----|--------|----------------|\n",
    "| 245 | tx_category | ENDOCRINOLOGY |\n",
    "| 367 | tx_category | CARDIOVASCULAR |\n",
    "| 412 | matched_conditions | [\"Diabetes Mellitus, Type 2 (MeSH ID:D003924)\"] |\n",
    "\n",
    "**CSV format:**\n",
    "```csv\n",
    "row,column,new_assignment\n",
    "245,tx_category,ENDOCRINOLOGY\n",
    "367,tx_category,CARDIOVASCULAR\n",
    "412,matched_conditions,\"[\"\"Diabetes Mellitus, Type 2 (MeSH ID:D003924)\"\"]\"\n",
    "```\n",
    "\n",
    "**Column explanations:**\n",
    "- `row`: The DataFrame index of the trial to modify (find using `annotated_trials[annotated_trials['nct_id'] == 'NCT12345678'].index[0]`)\n",
    "- `column`: Which column to modify (e.g., `tx_category`, `matched_conditions`, `tx_category_confidence`)\n",
    "- `new_assignment`: The corrected value (must match column data type)\n",
    "\n",
    "**Step 3: Run the Override Cell Below**\n",
    "\n",
    "The cell below will:\n",
    "1. Load your `human_review.csv` file\n",
    "2. Apply each correction to `annotated_trials`\n",
    "3. Log which changes were made\n",
    "\n",
    "```python\n",
    "# This is what the cell below does:\n",
    "for change in changes_df.itertuples():\n",
    "    row = change.row\n",
    "    column = change.column\n",
    "    new_assignment = change.new_assignment\n",
    "    \n",
    "    # Apply override\n",
    "    annotated_trials.loc[row, column] = new_assignment\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Example Workflow\n",
    "\n",
    "**Scenario:** You find that trial NCT04856789 (row 245) was categorized as NEPHROLOGY, but it's primarily about diabetes management in patients with kidney complications. The correct category should be ENDOCRINOLOGY.\n",
    "\n",
    "**Step-by-step:**\n",
    "\n",
    "1. **Find the row index:**\n",
    "```python\n",
    "row_idx = annotated_trials[annotated_trials['nct_id'] == 'NCT04856789'].index[0]\n",
    "print(f\"Row index: {row_idx}\")  # Output: 245\n",
    "```\n",
    "\n",
    "2. **Create `human_review.csv` in your data directory:**\n",
    "```csv\n",
    "row,column,new_assignment\n",
    "245,tx_category,ENDOCRINOLOGY\n",
    "245,tx_category_confidence,HIGH\n",
    "```\n",
    "(Note: You can update multiple columns for the same row, or multiple rows at once)\n",
    "\n",
    "3. **Save the CSV to:** `{DATA_STORAGE}/human_review.csv`\n",
    "\n",
    "4. **Run the cell below** ‚Üí Changes will be applied automatically\n",
    "\n",
    "5. **Verify the change:**\n",
    "```python\n",
    "print(annotated_trials.loc[245, 'tx_category'])  # Output: ENDOCRINOLOGY\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**‚úÖ DO:**\n",
    "- Document your rationale for overrides (add comments in CSV or separate notes)\n",
    "- Save a copy of `human_review.csv` for reproducibility\n",
    "\n",
    "**‚ùå DON'T:**\n",
    "- Override without inspecting the trial details\n",
    "- Batch-modify without verifying each case\n",
    "- Delete `human_review.csv` after applying (keep for audit trail)\n",
    "- Modify original data files directly\n",
    "\n",
    "---\n",
    "\n",
    "### What Can You Override?\n",
    "\n",
    "You can modify any column in the DataFrame, but most commonly:\n",
    "\n",
    "| Column | Type | Example Values |\n",
    "|--------|------|----------------|\n",
    "| `tx_category` | list | `[\"ENDOCRINOLOGY\"]` |\n",
    "| `matched_conditions` | list | `[\"Diabetes Mellitus, Type 2 (MeSH ID:D003924)\"]` |\n",
    "\n",
    "**Note:** When modifying list columns like `matched_conditions`, use proper JSON array syntax with escaped quotes in CSV:\n",
    "```csv\n",
    "row,column,new_assignment\n",
    "123,matched_conditions,\"[\"\"Diabetes Mellitus, Type 2 (MeSH ID:D003924)\"\", \"\"Obesity (MeSH ID:D009765)\"\"]\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Tracking Overrides\n",
    "\n",
    "After applying human overrides, you can track which trials were modified:\n",
    "\n",
    "```python\n",
    "# Load original automated annotations\n",
    "original = session.annotated_data.copy()\n",
    "\n",
    "# Compare to human-corrected version\n",
    "differences = []\n",
    "for idx in annotated_trials.index:\n",
    "    for col in ['tx_category', 'matched_conditions']:\n",
    "        if original.loc[idx, col] != annotated_trials.loc[idx, col]:\n",
    "            differences.append({\n",
    "                'nct_id': annotated_trials.loc[idx, 'nct_id'],\n",
    "                'column': col,\n",
    "                'llm_value': original.loc[idx, col],\n",
    "                'human_value': annotated_trials.loc[idx, col]\n",
    "            })\n",
    "\n",
    "override_log = pd.DataFrame(differences)\n",
    "override_log.to_csv(f\"{DATA_STORAGE}/human_overrides_log.csv\", index=False)\n",
    "print(f\"Logged {len(differences)} human overrides\")\n",
    "```\n",
    "\n",
    "This creates an audit trail showing:\n",
    "- Which trials were overridden\n",
    "- What the LLM originally assigned\n",
    "- What humans corrected it to\n",
    "\n",
    "---\n",
    "\n",
    "### When to Override vs Re-train\n",
    "\n",
    "**Override individual errors** when:\n",
    "- Small number of mistakes (<5% of trials)\n",
    "- Edge cases that are inherently ambiguous\n",
    "- Domain-specific judgments that LLM can't know\n",
    "\n",
    "**Improve LLM prompts/workflow** when:\n",
    "- Systematic errors (e.g., all diabetes trials miscategorized)\n",
    "- >10% of trials need correction\n",
    "- Clear pattern in mistakes (e.g., LLM consistently confuses two categories)\n",
    "\n",
    "In the latter case, modify prompts in `annotator.py` and re-run the workflow rather than manually fixing hundreds of trials.\n",
    "\n",
    "---\n",
    "\n",
    "## The Complete Human-LLM Partnership\n",
    "\n",
    "This override system exemplifies the \"circumscribed agentic\" philosophy:\n",
    "\n",
    "1. **LLM handles the bulk work** (synonym matching, context extraction, categorization)\n",
    "2. **Validation layers catch most errors** (schema checks, API verification, confidence scoring)\n",
    "3. **Humans review and correct edge cases** (domain expertise, nuanced judgment)\n",
    "4. **All decisions are auditable** (CSV exports, override logs, confidence scores)\n",
    "\n",
    "**Result:** A scalable, accurate, and trustworthy annotation pipeline that combines machine efficiency with human expertise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glp-1-landscape-nG0L0imC-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
